{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import csv\n",
    "import importlib\n",
    "from scripts import proj1_helpers, helpers\n",
    "from scripts import implementation, feature_processing, k_fold, model_logistic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path  = '../data/test.csv'\n",
    "output_path = '../data/logreg_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "y, X, idx = proj1_helpers.load_csv_data(train_path)\n",
    "y_t, X_t, ids_t = proj1_helpers.load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = feature_processing.process_X(X)\n",
    "X_t_p = feature_processing.process_X(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.random.randn(X_p.shape[1], 1)\n",
    "\n",
    "model = implementation.reg_logistic_regression\n",
    "model_args = {'initial_w': w0, 'max_iters': 100, 'gamma': 1e-5, 'debug': False}\n",
    "loss = model_logistic.reg_loss\n",
    "\n",
    "k_fold.cross_validation_select(X_p, y, model, loss, kw_model = model_args, seed = 1, k_fold = 5, N = 20, do_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_select(seed, degree, k_fold):\n",
    "    lambdas = np.logspace(-6, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(len(y), k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr, rmse_te = [], []\n",
    "    rmse = [rmse_tr, rmse_te]\n",
    "    rmse_all = [[], []]\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        rmse_ = [[], []]\n",
    "        for k in range(k_fold):\n",
    "            [rmse_[i].append(x) for i, x in\n",
    "             enumerate(cross_validation(y, x, k_indices, k, lambda_, degree))]\n",
    "        [rmse[i].append(np.mean(x)) for (i, x) in enumerate(rmse_)]\n",
    "        [rmse_all[i].append(x) for (i, x) in enumerate(rmse_)]\n",
    "    \n",
    "    idx_min = np.argmin(rmse_te)\n",
    "    \n",
    "    return idx_min, rmse_all, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(degrees, seed, k_fold):\n",
    "    lambdas = None\n",
    "    tr_losses = []\n",
    "    tr_losses_std = []\n",
    "    te_losses = []\n",
    "    te_losses_std = []\n",
    "    for degree in degrees:\n",
    "        idx_min, rmse_all, lambdas = cross_validation_select(seed, degree, k_fold)\n",
    "        print(lambdas,idx_min,lambdas[0]);\n",
    "        lambda_best = lambdas[idx_min]\n",
    "        print(lambda_best)\n",
    "        te_loss = np.mean(rmse_all[1][idx_min])\n",
    "        te_losses.append(te_loss)\n",
    "        te_loss_std = np.std(rmse_all[1][idx_min])\n",
    "        te_losses_std.append(te_loss_std)\n",
    "        tr_loss = np.mean(rmse_all[0][idx_min])\n",
    "        tr_losses.append(tr_loss)\n",
    "        tr_loss_std = np.std(rmse_all[0][idx_min])\n",
    "        tr_losses_std.append(tr_loss_std)\n",
    "        print(\"Degree %d Lambda %.5f Test loss %.3f +- %.3f Train loss %.3f +- %.3f\" %\n",
    "              (degree, lambda_best, te_loss, te_loss_std, tr_loss, tr_loss_std))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.errorbar(degrees, te_losses, yerr = te_losses_std, label = 'Train')\n",
    "    plt.errorbar(degrees, tr_losses, yerr = tr_losses_std, label = 'Train')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "get_best_models(np.arange(1), 1, 10)\n",
    "#get_best_models(np.arange(10), 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "Visualize bias-variance trade-off by implementing the function `bias_variance_demo()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from implementation import least_squares\n",
    "from split_data import split_data\n",
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def bias_variance_demo(ridge_lambda = 0, use_cv = 0):\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 5000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    k_fold = 4\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.zeros((len(seeds), len(degrees)))\n",
    "    rmse_te = np.zeros((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in tqdm(enumerate(seeds)):\n",
    "        np.random.seed(seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        \n",
    "        idx_tr, idx_te = split_data(x, y, ratio_train, seed = seed)\n",
    "        \n",
    "        for index_degree, degree in enumerate(degrees):\n",
    "            # building polynomial matrix for whole dataset\n",
    "            #x_poly = build_poly(x, degree)\n",
    "            y,tx = build_model_data(x,y)\n",
    "\n",
    "        \n",
    "            # x data: train/test\n",
    "            x_tr = tx[idx_tr, :]\n",
    "            x_te = tx[idx_te, :]\n",
    "    \n",
    "            # y data: train/test\n",
    "            y_tr = y[idx_tr]\n",
    "            y_te = y[idx_te]\n",
    "\n",
    "            if use_cv:\n",
    "                # selecting lambda:\n",
    "                idx_min, rmse_all, lambdas = cross_validation_select(np.array(x)[idx_tr], y_tr, seed, degree, k_fold)\n",
    "                ridge_lambda = lambdas[idx_min]\n",
    "            \n",
    "            # training ridge regression\n",
    "            if ridge_lambda > 0:\n",
    "                weights = ridge_regression(y_tr, x_tr, ridge_lambda)\n",
    "            else:\n",
    "                weights = least_squares(y_tr, x_tr)\n",
    "\n",
    "            # computing losses\n",
    "            rmse_tr[index_seed, index_degree] = compute_mse(y_tr, x_tr, weights)\n",
    "            rmse_te[index_seed, index_degree] = compute_mse(y_te, x_te, weights)\n",
    "\n",
    "    plt.figure()\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "    plt.boxplot(rmse_tr, positions = degrees)\n",
    "    plt.boxplot(rmse_te, positions = degrees)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# least squares\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ridge\n",
    "bias_variance_demo(ridge_lambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ridge with CV for lambda\n",
    "bias_variance_demo(use_cv = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import predict_labels\n",
    "from proj1_helpers import create_csv_submission\n",
    "\n",
    "def build_model(lambda_val,name):\n",
    "    X_t = impute_with_mean(x_test)\n",
    "    X_ts, _, _ = helpers.standardize(X_t)\n",
    "    y_t, tx_t = helpers.build_model_data(X_ts, y_test)\n",
    "    \n",
    "    y_m,tx = build_model_data(x,y)\n",
    "    weights = ridge_regression(y_m, tx, lambda_val)\n",
    "    \n",
    "    y_pred=predict_labels(weights,tx_t)\n",
    "    create_csv_submission(ids_test,y_pred,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_model(0.000001,\"submit_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
