{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import csv\n",
    "import importlib\n",
    "from scripts import proj1_helpers, helpers\n",
    "from scripts import implementation, feature_processing, k_fold, model_logistic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path  = '../data/test.csv'\n",
    "output_path = '../data/logreg_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "y, X, idx = proj1_helpers.load_csv_data(train_path)\n",
    "y_t, X_t, ids_t = proj1_helpers.load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.87s/it]\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "X_p = feature_processing.process_X(X)\n",
    "X_t_p = feature_processing.process_X(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f1cc12c80e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_logistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mk_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/repos/git/EPFL/ML/Project1/src/scripts/k_fold.py\u001b[0m in \u001b[0;36mcross_validation_select\u001b[0;34m(x, y, model, loss, kw_model, kw_loss, seed, k_fold, N, do_plot)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             [rmse_[i].append(x) for i, x in\n\u001b[0;32m---> 56\u001b[0;31m              enumerate(cross_validation(y, x, k_indices, k, model, kw_model, loss, kw_loss, lambda_))]\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mrmse_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/git/EPFL/ML/Project1/src/scripts/k_fold.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, tx, k_indices, k, model, kw_model, loss, kw_loss, lambda_)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# training ridge regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# computing losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/git/EPFL/ML/Project1/src/scripts/implementation.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma, debug)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\" implement regularized logistic regression via gradient descent \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_logistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_logistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lambda_'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_last_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/git/EPFL/ML/Project1/src/scripts/helpers.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma, loss_f, grad_f, kwargs, debug)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# calculating loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/git/EPFL/ML/Project1/src/scripts/model_logistic.py\u001b[0m in \u001b[0;36mreg_loss\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\" returns regularized logistic regression loss \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/repos/git/EPFL/ML/Project1/src/scripts/model_logistic.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"\"\" returns logistic regression loss \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreg_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w0 = np.random.randn(X_p.shape[1], 1)\n",
    "\n",
    "model = implementation.reg_logistic_regression\n",
    "model_args = {'initial_w': w0, 'max_iters': 100, 'gamma': 1e-5, 'debug': False}\n",
    "loss = model_logistic.reg_loss\n",
    "\n",
    "k_fold.cross_validation_select(X_p, y, model, loss, kw_model = model_args, seed = 1, k_fold = 5, N = 20, do_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_select(seed, degree, k_fold):\n",
    "    lambdas = np.logspace(-6, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(len(y), k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr, rmse_te = [], []\n",
    "    rmse = [rmse_tr, rmse_te]\n",
    "    rmse_all = [[], []]\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        rmse_ = [[], []]\n",
    "        for k in range(k_fold):\n",
    "            [rmse_[i].append(x) for i, x in\n",
    "             enumerate(cross_validation(y, x, k_indices, k, lambda_, degree))]\n",
    "        [rmse[i].append(np.mean(x)) for (i, x) in enumerate(rmse_)]\n",
    "        [rmse_all[i].append(x) for (i, x) in enumerate(rmse_)]\n",
    "    \n",
    "    idx_min = np.argmin(rmse_te)\n",
    "    \n",
    "    return idx_min, rmse_all, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-06   1.61026203e-06   2.59294380e-06   4.17531894e-06\n",
      "   6.72335754e-06   1.08263673e-05   1.74332882e-05   2.80721620e-05\n",
      "   4.52035366e-05   7.27895384e-05   1.17210230e-04   1.88739182e-04\n",
      "   3.03919538e-04   4.89390092e-04   7.88046282e-04   1.26896100e-03\n",
      "   2.04335972e-03   3.29034456e-03   5.29831691e-03   8.53167852e-03\n",
      "   1.37382380e-02   2.21221629e-02   3.56224789e-02   5.73615251e-02\n",
      "   9.23670857e-02   1.48735211e-01   2.39502662e-01   3.85662042e-01\n",
      "   6.21016942e-01   1.00000000e+00] 0 1e-06\n",
      "1e-06\n",
      "Degree 0 Lambda 0.00000 Test loss 0.341 +- 0.002 Train loss 0.341 +- 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3800195048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_best_models(degrees, seed, k_fold):\n",
    "    lambdas = None\n",
    "    tr_losses = []\n",
    "    tr_losses_std = []\n",
    "    te_losses = []\n",
    "    te_losses_std = []\n",
    "    for degree in degrees:\n",
    "        idx_min, rmse_all, lambdas = cross_validation_select(seed, degree, k_fold)\n",
    "        print(lambdas,idx_min,lambdas[0]);\n",
    "        lambda_best = lambdas[idx_min]\n",
    "        print(lambda_best)\n",
    "        te_loss = np.mean(rmse_all[1][idx_min])\n",
    "        te_losses.append(te_loss)\n",
    "        te_loss_std = np.std(rmse_all[1][idx_min])\n",
    "        te_losses_std.append(te_loss_std)\n",
    "        tr_loss = np.mean(rmse_all[0][idx_min])\n",
    "        tr_losses.append(tr_loss)\n",
    "        tr_loss_std = np.std(rmse_all[0][idx_min])\n",
    "        tr_losses_std.append(tr_loss_std)\n",
    "        print(\"Degree %d Lambda %.5f Test loss %.3f +- %.3f Train loss %.3f +- %.3f\" %\n",
    "              (degree, lambda_best, te_loss, te_loss_std, tr_loss, tr_loss_std))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.errorbar(degrees, te_losses, yerr = te_losses_std, label = 'Train')\n",
    "    plt.errorbar(degrees, tr_losses, yerr = tr_losses_std, label = 'Train')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "get_best_models(np.arange(1), 1, 10)\n",
    "#get_best_models(np.arange(10), 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "Visualize bias-variance trade-off by implementing the function `bias_variance_demo()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from implementation import least_squares\n",
    "from split_data import split_data\n",
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def bias_variance_demo(ridge_lambda = 0, use_cv = 0):\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 5000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    k_fold = 4\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.zeros((len(seeds), len(degrees)))\n",
    "    rmse_te = np.zeros((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in tqdm(enumerate(seeds)):\n",
    "        np.random.seed(seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        \n",
    "        idx_tr, idx_te = split_data(x, y, ratio_train, seed = seed)\n",
    "        \n",
    "        for index_degree, degree in enumerate(degrees):\n",
    "            # building polynomial matrix for whole dataset\n",
    "            #x_poly = build_poly(x, degree)\n",
    "            y,tx = build_model_data(x,y)\n",
    "\n",
    "        \n",
    "            # x data: train/test\n",
    "            x_tr = tx[idx_tr, :]\n",
    "            x_te = tx[idx_te, :]\n",
    "    \n",
    "            # y data: train/test\n",
    "            y_tr = y[idx_tr]\n",
    "            y_te = y[idx_te]\n",
    "\n",
    "            if use_cv:\n",
    "                # selecting lambda:\n",
    "                idx_min, rmse_all, lambdas = cross_validation_select(np.array(x)[idx_tr], y_tr, seed, degree, k_fold)\n",
    "                ridge_lambda = lambdas[idx_min]\n",
    "            \n",
    "            # training ridge regression\n",
    "            if ridge_lambda > 0:\n",
    "                weights = ridge_regression(y_tr, x_tr, ridge_lambda)\n",
    "            else:\n",
    "                weights = least_squares(y_tr, x_tr)\n",
    "\n",
    "            # computing losses\n",
    "            rmse_tr[index_seed, index_degree] = compute_mse(y_tr, x_tr, weights)\n",
    "            rmse_te[index_seed, index_degree] = compute_mse(y_te, x_te, weights)\n",
    "\n",
    "    plt.figure()\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "    plt.boxplot(rmse_tr, positions = degrees)\n",
    "    plt.boxplot(rmse_te, positions = degrees)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# least squares\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ridge\n",
    "bias_variance_demo(ridge_lambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ridge with CV for lambda\n",
    "bias_variance_demo(use_cv = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import predict_labels\n",
    "from proj1_helpers import create_csv_submission\n",
    "\n",
    "def build_model(lambda_val,name):\n",
    "    X_t = impute_with_mean(x_test)\n",
    "    X_ts, _, _ = helpers.standardize(X_t)\n",
    "    y_t, tx_t = helpers.build_model_data(X_ts, y_test)\n",
    "    \n",
    "    y_m,tx = build_model_data(x,y)\n",
    "    weights = ridge_regression(y_m, tx, lambda_val)\n",
    "    \n",
    "    y_pred=predict_labels(weights,tx_t)\n",
    "    create_csv_submission(ids_test,y_pred,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_model(0.000001,\"submit_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
