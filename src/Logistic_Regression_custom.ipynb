{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./scripts\")\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from feature_processing import *\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataPath):\n",
    "\n",
    "    yb , input_data, ids = load_csv_data(dataPath) # load data\n",
    "    tx =  process_X(input_data)\n",
    "    \n",
    "    return ids,yb,tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ids_train,y_train,tx_train = prepare_data('../data/train.csv')\n",
    "ids_test,y_test,tx_test =  prepare_data('../data/test.csv')\n",
    "print(tx_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    numerator = np.exp(t)\n",
    "    denominator = np.add(1,numerator)\n",
    "    return (numerator/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \n",
    "    A = (np.exp(tx.dot(w))) + 1 \n",
    "    mult = np.dot((np.dot(y.T,tx)),w)\n",
    "    loss_vector = (np.log(A)) - mult\n",
    "    loss_component = np.sum(loss_vector)\n",
    "    return loss_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "   \n",
    "    step1 = sigmoid(tx.dot(w)) - y\n",
    "    step2 = tx.T\n",
    "    gradient = np.dot(step2,step1)\n",
    "    return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    w_s = w\n",
    "   \n",
    "    w_s = w_s - gamma *  calculate_gradient(y, tx, w_s)\n",
    "    loss =  calculate_loss(y, tx, w_s)  \n",
    "    \n",
    "    return loss,w_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.000001\n",
    "    losses = []\n",
    "\n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, x, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0 and len(losses) > 1:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=np.abs(losses[-1] - losses[-2])))\n",
    "            \n",
    "            \n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=4659286.7527832985\n",
      "Current iteration=200, loss=8335007.398172379\n",
      "Current iteration=300, loss=5427126.829332352\n",
      "Current iteration=400, loss=4022302.8496751785\n",
      "Current iteration=500, loss=3162457.276881218\n",
      "Current iteration=600, loss=2597581.432374954\n",
      "Current iteration=700, loss=2201783.690522194\n",
      "Current iteration=800, loss=1910784.9320440292\n",
      "Current iteration=900, loss=1687828.387383461\n",
      "Current iteration=1000, loss=1510899.0029821396\n",
      "Current iteration=1100, loss=1366498.5466079712\n",
      "Current iteration=1200, loss=1246081.0660581589\n",
      "Current iteration=1300, loss=1143985.993675232\n",
      "Current iteration=1400, loss=1056272.7476530075\n",
      "Current iteration=1500, loss=980080.0049915314\n",
      "Current iteration=1600, loss=913265.1281394958\n",
      "Current iteration=1700, loss=854187.7504796982\n",
      "Current iteration=1800, loss=801570.2799119949\n",
      "Current iteration=1900, loss=754403.0010118484\n",
      "Current iteration=2000, loss=711877.4132528305\n",
      "Current iteration=2100, loss=673338.5121927261\n",
      "Current iteration=2200, loss=638250.0721492767\n",
      "Current iteration=2300, loss=606168.9189147949\n",
      "Current iteration=2400, loss=576725.5102434158\n",
      "Current iteration=2500, loss=549609.065823555\n",
      "Current iteration=2600, loss=524556.0824842453\n",
      "Current iteration=2700, loss=501341.4221096039\n",
      "Current iteration=2800, loss=479771.3694572449\n",
      "Current iteration=2900, loss=459678.1961364746\n",
      "Current iteration=3000, loss=440915.8707752228\n",
      "Current iteration=3100, loss=423356.6399154663\n",
      "Current iteration=3200, loss=406888.27135276794\n",
      "Current iteration=3300, loss=391411.80521965027\n",
      "Current iteration=3400, loss=376839.69835472107\n",
      "Current iteration=3500, loss=363094.2774372101\n",
      "Current iteration=3600, loss=350106.4377403259\n",
      "Current iteration=3700, loss=337814.54036712646\n",
      "Current iteration=3800, loss=326163.47172927856\n",
      "Current iteration=3900, loss=315103.8370246887\n",
      "Current iteration=4000, loss=304591.265750885\n",
      "Current iteration=4100, loss=294585.81145477295\n",
      "Current iteration=4200, loss=285051.43120384216\n",
      "Current iteration=4300, loss=275955.5330963135\n",
      "Current iteration=4400, loss=267268.58197784424\n",
      "Current iteration=4500, loss=258963.75503730774\n",
      "Current iteration=4600, loss=251016.64065170288\n",
      "Current iteration=4700, loss=243404.97418022156\n",
      "Current iteration=4800, loss=236108.40641212463\n",
      "Current iteration=4900, loss=229108.29972839355\n",
      "Current iteration=5000, loss=222387.5487689972\n",
      "Current iteration=5100, loss=215930.42236328125\n",
      "Current iteration=5200, loss=209722.42391586304\n",
      "Current iteration=5300, loss=203750.1681251526\n",
      "Current iteration=5400, loss=198001.27174186707\n",
      "Current iteration=5500, loss=192464.2568397522\n",
      "Current iteration=5600, loss=187128.46501159668\n",
      "Current iteration=5700, loss=181983.9809741974\n",
      "Current iteration=5800, loss=177021.56469345093\n",
      "Current iteration=5900, loss=172232.59091186523\n",
      "Current iteration=6000, loss=167608.99510383606\n",
      "Current iteration=6100, loss=163143.22519493103\n",
      "Current iteration=6200, loss=158828.1983013153\n",
      "Current iteration=6300, loss=154657.26203346252\n",
      "Current iteration=6400, loss=150624.1595993042\n",
      "Current iteration=6500, loss=146722.998544693\n",
      "Current iteration=6600, loss=142948.22243881226\n",
      "Current iteration=6700, loss=139294.58540344238\n",
      "Current iteration=6800, loss=135757.12908935547\n",
      "Current iteration=6900, loss=132331.1617488861\n",
      "Current iteration=7000, loss=129012.23924255371\n",
      "Current iteration=7100, loss=125796.14781188965\n",
      "Current iteration=7200, loss=122678.88838005066\n",
      "Current iteration=7300, loss=119656.66216278076\n",
      "Current iteration=7400, loss=116725.85759544373\n",
      "Current iteration=7500, loss=113883.03832054138\n",
      "Current iteration=7600, loss=111124.93216133118\n",
      "Current iteration=7700, loss=108448.42095947266\n",
      "Current iteration=7800, loss=105850.53131103516\n",
      "Current iteration=7900, loss=103328.42589378357\n",
      "Current iteration=8000, loss=100879.39559364319\n",
      "Current iteration=8100, loss=98500.85211753845\n",
      "Current iteration=8200, loss=96190.32117271423\n",
      "Current iteration=8300, loss=93945.436170578\n",
      "Current iteration=8400, loss=91763.93233680725\n",
      "Current iteration=8500, loss=89643.64122200012\n",
      "Current iteration=8600, loss=87582.48566246033\n",
      "Current iteration=8700, loss=85578.47497940063\n",
      "Current iteration=8800, loss=83629.70051193237\n",
      "Current iteration=8900, loss=81734.33152198792\n",
      "Current iteration=9000, loss=79890.61123847961\n",
      "Current iteration=9100, loss=78096.85322761536\n",
      "Current iteration=9200, loss=76351.43795204163\n",
      "Current iteration=9300, loss=74652.8095703125\n",
      "Current iteration=9400, loss=72999.47286224365\n",
      "Current iteration=9500, loss=71389.99039840698\n",
      "Current iteration=9600, loss=69822.9798488617\n",
      "Current iteration=9700, loss=68297.11137580872\n",
      "Current iteration=9800, loss=66811.10538291931\n",
      "Current iteration=9900, loss=65363.73006629944\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "y_train = y_train.reshape((len(y_train),1))\n",
    "y_train[y_train == -1.0] = 0.0\n",
    "\n",
    "loss,w = logistic_regression_gradient_descent_demo(y_train, tx_train)\n",
    "y_pred = predict_labels(w, tx_test)\n",
    "create_csv_submission(ids_test, y_pred,'../data/baseline_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
